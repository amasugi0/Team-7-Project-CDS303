{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering algorithm overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook, we will explore clustering. Remember, clustering is an unsupervised method to identify potential subpopulations within a population, based on combinations of attributes. We can then go on to use the resulting model for supervised classification tasks. This notebook will take you through that process from start to finish. In the process of covering clustering methods, we will also cover topics such as:\n",
    "\n",
    "- visualizing results\n",
    "- evaluating performance with metrics and graphs\n",
    "- setting up and running experiments\n",
    "- parameter grid spaces\n",
    "- joining lists\n",
    "- hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is unsupervised, meaning we do not have a goal. So, with this dataset, we have four variables in 'quant_data' and all of them are 'X'. We're not trying to predict / classify according to any one of these features right now, we just want to see if there are any subpopulations. Then we'll look to see if the patterns we found match up to any of the potential classifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is, there are may different algorithms to do clustering, but they are generally divided into four groups based on the way they perform clustering: centroid (e.g. K-means), distribution (e.g. Gaussian Mixture), connectivity (e.g. Hierarchical), and density (e.g. DBSCAN). We have included three of those here, plus Affinity Propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical - Agglomerative (connectivity):\n",
    "\n",
    "Hierarchical clustering is a family of algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Agglomerative Clustering performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans (centroid): \n",
    "Clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. \n",
    "- Choose the initial centroids based on km_init and n_clusters\n",
    "- Assign each sample to nearest centroid\n",
    "- Take mean of all samples in each cluster\n",
    "- Choose new centroid based on these means, based on algorithm (lloyd vs. elkan)\n",
    "- Continue until until centroids stop moving or stop at max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN (density): \n",
    "\n",
    "Clusters data by density of data points. Unlike Kmeans, it is insensitive to cluster shape. Core samples are in high density areas. A cluster is a set of core samples that can be built by \n",
    "\n",
    "- recursively taking a core sample, \n",
    "- finding all of its neighbors that are core samples, \n",
    "- finding all of their neighbors that are core samples, \n",
    "- and so on. \n",
    "\n",
    "A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples (e.g. on fringes of clusters). Any non-core sample at least eps from a core sample is considered an outlier. Noisy samples are labelled -1, so we want to minimize the number of samples with label = -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity Propogation (relatively new approach):\n",
    "\n",
    "Creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. Basically, it is like social group formation: how desirable is it to join a group, and how much space is there for you to join.\n",
    "\n",
    "- The messages sent between pairs represent the suitability for one sample to be the exemplar of the other. \n",
    "- Updates iteratively until convergence, final exemplars are chosen, and final clustering is given. \n",
    "- The messages sent between points belong to one of two categories. \n",
    "    - Responsibility r(i,k): accumulated evidence that sample k should be the exemplar for sample i. \n",
    "    - Availability a(i,k): accumulated evidence that sample a should choose sample k to be its exemplar. \n",
    "\n",
    "To evaluate performance, we can look at number of iterations needed to converge, compared to total number of iterations. Sometimes, the algorithm doesn't converge meaning that set of parameters wasn't very good. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering: running and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because this is unsupervised, we will not be specifying 'y' (a target variable). We'll go through the code first, then describe what each model type is doing later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import pandas as pd  \n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn \n",
    "from sklearn.cluster import KMeans, AffinityPropagation, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've done for other examples, we are using the penguins dataset, and filling in missing values. We then normalize the quantitative data so that all values are between 0 and 1, resulting in the 'data' dataframe which we'll use for our clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data that is built into seaborn\n",
    "data = sns.load_dataset('penguins')\n",
    "\n",
    "# Fill in missing values. There are many other things we can / should do to clean up the data, but here we are just\n",
    "# doing the absolute minimum necessary to proceed.\n",
    "data['bill_length_mm'].fillna(data['bill_length_mm'].mean(), inplace=True)\n",
    "data['bill_depth_mm'].fillna(data['bill_depth_mm'].mean(), inplace=True)\n",
    "data['flipper_length_mm'].fillna(data['flipper_length_mm'].mean(), inplace=True)\n",
    "data['body_mass_g'].fillna(data['body_mass_g'].mean(), inplace=True)\n",
    "data['sex'].fillna(data['sex'].value_counts().index[0], inplace=True)\n",
    "\n",
    "# Scale the data so that each column is between 0 and 1, to ensure large numbers don't have too much of an overstated effect.\n",
    "scaler = MinMaxScaler()\n",
    "data_norm = scaler.fit_transform(data[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]])\n",
    "norm_names = scaler.get_feature_names_out()\n",
    "data = pd.DataFrame(data_norm, columns=norm_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dictionaries\n",
    "\n",
    "Rather than creating a big block of code for each model type, we're going to set up dictionaries of models and their parameters, so that we can have one block of code iterate through the dictionaries. Note that the parameter dictionary is actually a nested one, and includes the ranges of parameter values for calculating the parameter grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPER-PARAMETERS:\n",
    "- HC - affinity = how algo will calculate distances, n_clusters = # of clusters to find\n",
    "- KM - init = how initial centroids are chosen, n_clusters = # of clusters to find\n",
    "- DB - eps = max distance apart for two samples to be in same cluster, min_samples = # samples in a neighborhood for point to be core\n",
    "- AF - damping = smooths out the effect of the messages passed, convergence_iter = # iterations to run after no change in clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'hc' : AgglomerativeClustering(),\n",
    "    'km' : KMeans(),\n",
    "    'db' : DBSCAN(),\n",
    "    'af' : AffinityPropagation()\n",
    "    }\n",
    "\n",
    "param_dict = {\n",
    "    'hc' : ('affinity', ['euclidean'], 'n_clusters', np.round(np.linspace(2,20, 5),2)),\n",
    "    'km' : ('init', [\"k-means++\", \"random\"], 'n_clusters', np.round(np.linspace(2,20, 5),2)),\n",
    "    'db' : ('eps', np.linspace(.1, .3, 3), 'min_samples', np.round(np.linspace(2, 8, 6),2)),\n",
    "    'af' : ('damping', np.round(np.linspace(0.5, 0.9, 5),2), 'convergence_iter', np.round(np.linspace(5, 20, 5),2))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models\n",
    "\n",
    "OK, now we're ready to run the models. Basically, for each of the four model types, we'll build a parameter grid, then use each set of parameters from the grid to update the model parameter dictionary and run the model on 'data'. We'll calculate the silhouette and davies-bouldin scores for each:\n",
    "- silhouette score: difference between a sample and points in the same cluster, vs. points in the next nearest cluster, higher is better, drawback is that it is generally higher for density-based clusters\n",
    "- davies-bouldin score: similarity between clusters, lower is better, same drawback as silhouette score.\n",
    "Then build a table summarizing the scores for each parameter combo, for each model. Finally, we'll filter down to the best scores for our metrics. Note, each of these model types has their own specific metrics, but for now I'd like to just compare across all four so I'm sticking with the common metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>param1</th>\n",
       "      <th>parval1</th>\n",
       "      <th>param2</th>\n",
       "      <th>parval2</th>\n",
       "      <th>sil score</th>\n",
       "      <th>dav score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hc</td>\n",
       "      <td>affinity</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>n_clusters</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.548285</td>\n",
       "      <td>0.684308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>km</td>\n",
       "      <td>init</td>\n",
       "      <td>k-means++</td>\n",
       "      <td>n_clusters</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.548285</td>\n",
       "      <td>0.684308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>km</td>\n",
       "      <td>init</td>\n",
       "      <td>random</td>\n",
       "      <td>n_clusters</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.548285</td>\n",
       "      <td>0.684308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>db</td>\n",
       "      <td>eps</td>\n",
       "      <td>0.2</td>\n",
       "      <td>min_samples</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.462908</td>\n",
       "      <td>0.603933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model    param1    parval1       param2  parval2  sil score  dav score\n",
       "21    db       eps        0.2  min_samples      2.0   0.462908   0.603933\n",
       "22    db       eps        0.2  min_samples      3.2   0.462908   0.603933\n",
       "23    db       eps        0.2  min_samples      4.4   0.462908   0.603933\n",
       "24    db       eps        0.2  min_samples      5.6   0.462908   0.603933\n",
       "25    db       eps        0.2  min_samples      6.8   0.462908   0.603933\n",
       "0     hc  affinity  euclidean   n_clusters      2.0   0.548285   0.684308\n",
       "5     km      init  k-means++   n_clusters      2.0   0.548285   0.684308\n",
       "10    km      init     random   n_clusters      2.0   0.548285   0.684308\n",
       "21    db       eps        0.2  min_samples      2.0   0.462908   0.603933\n",
       "22    db       eps        0.2  min_samples      3.2   0.462908   0.603933"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = []\n",
    "nbest = 5\n",
    "for i in ['hc', 'km', 'db', 'af']:\n",
    "    param_grid = list(itertools.product(param_dict[i][1], param_dict[i][3]))\n",
    "    for k in range(len(param_grid)):\n",
    "        param_grid_dict = {param_dict[i][0] : param_grid[k][0], param_dict[i][2] : int(param_grid[k][1])}\n",
    "        model = model_dict[i]\n",
    "        model.set_params(**param_grid_dict)\n",
    "        model.fit(data)\n",
    "        sil_score = silhouette_score(data, model.labels_)\n",
    "        dav_score = davies_bouldin_score(data, model.labels_)\n",
    "        summary.append([i, param_dict[i][0], param_grid[k][0], param_dict[i][2], param_grid[k][1], sil_score, dav_score])\n",
    "summary_df = pd.DataFrame(summary, columns=('model', 'param1', 'parval1', 'param2', 'parval2', 'sil score', 'dav score'))\n",
    "best_df = pd.concat([\n",
    "    (summary_df.nsmallest(nbest, 'dav score')), \n",
    "    (summary_df.nlargest(nbest, 'sil score'))], axis=0)\n",
    "best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have our 10 best model/parameter combos, based on silhouette and davies-bouldin scores. We varied two parameters for each model: this allowed us to keep our code very short and simple. You can see the table has the parameters and their values (param1, parval1, param2, parval2), plus the model type and metric scores. We're looking to maximize silhouette and minimize davies-bouldin. DBSCAN was the most common model appearing in our top 10. The best silhouette scores though were not DBSCAN, but the best davies-bouldin scores were. This is interesting, because generally both the silhouette and davies-bouldin scores are higher for density-based clustering. So, in this case, I would be inclined to trust the silhouette results that point to kmeans and hierarchical clustering as being better. You could plot each one of these and take a look to see how well they seem to be clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try plotting the results above, coloring by assigned cluster value, to see if you can discover any other insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7edd4e905d9b7ca730f7b0e9c082715bdac78ae21b6c7d0ae05faf51bf553083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
